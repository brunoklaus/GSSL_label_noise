@article{wang2016trend,
  title={Trend filtering on graphs},
  author={Wang, Yu-Xiang and Sharpnack, James and Smola, Alexander J and Tibshirani, Ryan J},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={3651--3691},
  year={2016},
  publisher={JMLR. org}
}

@mastersthesis{afonsothesis2020,
    author = {Afonso, Bruno Klaus},
    institution = {Dissertação (Mestrado) - UNIFESP},
    title = {Analysis of Label Noise in Graph-Based Semi-Supervised Learning},
    year = 2020
}
@article{belkin2003,
abstract = {A major challenge in the post-genomic era is to determine protein function on a proteomic scale. There are only less than half of the actual functional annotations available for a typical proteome. The recent high-throughput bio-techniques have provided us large-scale protein-protein interaction data, and many studies have shown that function prediction from protein-protein interaction data is a promising way since proteins are likely to collaborate for a common purpose. However, the protein interaction data is very noisy, which makes the task very challenging. In this paper, we propose a distance matrix based on the small world property of the protein-protein interaction network. It measures the reliability of edges and filter the noise in the network. In addition, an ANN (Artificial Neural Network) based method was designed to predict protein functions on the basis of integration of several protein interaction data sets. We tested our approach with MIPS functional categories. The experiential results show that our approach has better performance than other existing methods in terms of precision and recall.},
author = {Belkin, M. and Niyogi, P.},
journal = {Advances in Neural Information Processing Systems},
pages = {929--936},
title = {{Using manifold structure for partially labeled classification}},
volume = {15},
year = {2003}
}



@article{Zhu2002,
abstract = {We investigate the use of unlabeled data to help labeled data in classification. We propose a simple iterative algorithm, label propagation, to propagate labels through the dataset along high density areas defined by unlabeled data. We analyze the algorithm, show its solution, and its connection to several other algorithms. We also show how to learn parameters by minimum spanning tree heuristic and entropy minimization, and the algorithms ability to perform feature selection. Experiment results are promising.},
author = {Zhu and Ghahramani, Z},
journal = {School Comput Sci Carnegie Mellon Univ Pittsburgh PA Tech Rep CMUCALD02107},
keywords = {learning},
number = {CMU-CALD-02-107},
pages = {1--19},
title = {{Learning from labeled and unlabeled data with label propagation}},
volume = {54},
year = {2002}
}

@article{Baluja2008,
author = {Baluja, Shumeet and Seth, Rohan and Sivakumar, D. and Jing, Yushi and Yagnik, Jay and Kumar, Shankar and Ravichandran, Deepak and Aly, Mohamed},
journal = {Proceeding of the 17th international conference on World Wide Web  - WWW '08},
keywords = {collaborative,filtering,label propagation,random walks,recommendation systems,video search},
pages = {895},
title = {{Video suggestion and discovery for youtube}},
year = {2008}
}
@inproceedings{wang2009label,
  title={Label diagnosis through self tuning for web image search},
  author={Wang, Jun and Jiang, Yu-Gang and Chang, Shih-Fu},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1390--1397},
  year={2009},
  organization={IEEE}
}
@techreport{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex},
  year={2009},
  institution={Citeseer}
}


@inproceedings{Zhu_2003,
 author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
 title = {Semi-supervised Learning Using Gaussian Fields and Harmonic Functions},
 booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
 year = {2003},
 location = {Washington, DC, USA},
 pages = {912--919},
 numpages = {8},
 publisher = {AAAI Press}
} 
@article{Wang2008,
author = {Wang, Jun and Jebara, Tony and Chang, Shih-Fu},
doi = {10.1145/1390156.1390300},
isbn = {9781605582054},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
mendeley-groups = {TCC/DeepSSL},
pages = {1144--1151},
title = {{Graph transduction via alternating minimization}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390300},
year = {2008}
}


@INPROCEEDINGS{Zhou_etall_2004,
    author = {Dengyong Zhou and Olivier Bousquet and Thomas Navin Lal and Jason Weston and Bernhard Schölkopf},
    title = {Learning with local and global consistency},
    booktitle = {Advances in Neural Information Processing Systems 16},
    year = {2004},
    pages = {321--328},
    publisher = {MIT Press}
}

@ARTICLE{Wang_Zhang_2008,
   author = {Wang, F. and Zhang, C.},
   title = {Label propagation through linear neighborhoods},
   journal = {IEEE Transactions on Knowledge and Data Enginineering},
   year = {2008},
   volume = {20},
   pages = {55--67}
}

@TECHREPORT{Zhu_2005,
 author = {Xiaojin Zhu},
 title = {Semi-supervised learning literature survey},
 institution = {University of Wisconsin-Madison},
 year = {2005},
 number = {1530},
 note = {Computer Sciences}
}

@inproceedings{Talukdar_2009,
 author = {Talukdar, Partha Pratim and Crammer, Koby},
 title = {New Regularized Algorithms for Transductive Learning},
 booktitle = {Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II},
 series = {ECML PKDD '09},
 year = {2009},
 location = {Bled, Slovenia},
 pages = {442--457},
 numpages = {16},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg}
} 

@inproceedings{Jebara_etall_2009,
 author = {Jebara, Tony and Wang, Jun and Chang, Shih-Fu},
 title = {Graph Construction and B-matching for Semi-supervised Learning},
 booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
 series = {ICML '09},
 year = {2009},
 location = {Montreal, Quebec, Canada},
 pages = {441--448},
 numpages = {8},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@Article{Faleiros_2016,
  Title = {Optimizing the class information divergence for transductive classification of texts using propagation in bipartite graphs},
  Author = {Thiago de Paulo Faleiros and Rafael Rossi and Alneu de Andrade Lopes},
  Journal = {Pattern Recognition Letters},
  Year = {2016},
  Pages = {1--12},
  Volume = {000}
}

@article{Vega-Oliveros_etall_2013,
  author={Didier A Vega-Oliveros and Lilian Berton and Andre Mantini Eberle and Alneu de Andrade Lopes and Liang Zhao},
  title={Regular graph construction for semi-supervised learning},
  journal={Journal of Physics: Conference Series},
  volume={490},
  number={1},
  pages={012022},
  year={2014}
}

@INPROCEEDINGS{Berton_2015, 
author={Lilian Berton and Jorge Valverde-Rebaza and Alneu de Andrade Lopes}, 
booktitle={2015 International Joint Conference on Neural Networks (IJCNN)}, 
title={Link prediction in graph construction for supervised and semi-supervised learning}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}  
}

@article{Berton_2017,
title = "RGCLI: Robust Graph that Considers Labeled Instances for Semi-Supervised Learning",
journal = "Neurocomputing",
volume = "226",
pages = "238 - 248",
year = "2017",
author = "Lilian Berton and Thiago de Paulo Faleiros and Alan Valejo and Jorge Valverde-Rebaza and Alneu de Andrade Lopes",
keywords = "Graph construction, -nearest neighbors, Semi-supervised learning, Label propagation"
}


@book{Chapelle_etal_2010,
 author = {Chapelle, Olivier and Schlkopf, Bernhard and Zien, Alexander},
 title = {Semi-Supervised Learning},
 year = {2010},
 edition = {1st},
 publisher = {The MIT Press}
} 

@misc{Dua:2017,
author = "Dheeru, Dua and Karra Taniskidou, Efi",
year = "2017",
title = "{UCI} Machine Learning Repository",
institution = "University of California, Irvine, School of Information and Computer Sciences",
url = "http://archive.ics.uci.edu/ml"
}


@Book{Mitchell97,
	author = {T.M. Mitchell},
	title = {Machine Learning},
	publisher = {McGraw-Hill},
	address = {New York},
	keywords = {machine learning},
	year = {1997}
}

@article{Frenay_2014,
  title={Classification in the Presence of Label Noise: A Survey},
  author={Beno{\^i}t Fr{\'e}nay and Michel Verleysen},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2014},
  volume={25},
  pages={845-869}
}

@article{Lewis_2004,
 author = {Lewis, David D. and Yang, Yiming and Rose, Tony G. and Li, Fan},
 title = {RCV1: A New Benchmark Collection for Text Categorization Research},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2004},
 volume = {5},
 month = dec,
 year = {2004},
 pages = {361--397},
 numpages = {37},
 acmid = {1005345},
 publisher = {JMLR.org},
} 

@article{Malossini_2006,
author = {Malossini, Andrea and Blanzieri, Enrico and Ng, Raymond T.},
title = {Detecting potential labeling errors in microarrays by data perturbation},
journal = {Bioinformatics},
volume = {22},
number = {17},
pages = {2114-2121},
year = {2006}
}

@article{Hickey_1996,
author = {R. J. Hickey},
title = {Noise modelling and evaluating learning from examples},
journal = {Artificial Intelligence},
volume = {82},
number = {1},
pages = {157-179},
year = {1996}
}

@incollection{Guyon_1996,
  author    = {Isabelle Guyon and
               Nada Matic and
               Vladimir Vapnik},
  title     = {Discovering Informative Patterns and Data Cleaning},
  booktitle = {Advances in Knowledge Discovery and Data Mining},
  pages     = {181--203},
  year      = {1996}
}

@Article{Angluin1988,
author="Angluin, Dana and Laird, Philip",
title="Learning From Noisy Examples",
journal="Machine Learning",
year="1988",
month="Apr",
day="01",
volume="2",
number="4",
pages="343--370",
abstract="The basic question addressed in this paper is: how can a learning algorithm cope with incorrect training examples? Specifically, how can algorithms that produce an ``approximately correct'' identification with ``high probability'' for reliable data be adapted to handle noisy data? We show that when the teacher may make independent random errors in classifying the example data, the strategy of selecting the most consistent rule for the sample is sufficient, and usually requires a feasibly small number of examples, provided noise affects less than half the examples on average. In this setting we are able to estimate the rate of noise using only the knowledge that the rate is less than one half. The basic ideas extend to other types of random noise as well. We also show that the search problem associated with this strategy is intractable in general. However, for particular classes of rules the target rule may be efficiently identified if we use techniques specific to that class. For an important class of formulas -- the k-CNF formulas studied by Valiant -- we present a polynomial-time algorithm that identifies concepts in this form when the rate of classification errors is less than one half."
}


@Article{Zhu2004,
author="Zhu, Xingquan
and Wu, Xindong",
title="Class Noise vs. Attribute Noise: A Quantitative Study",
journal="Artificial Intelligence Review",
year="2004",
month="Nov",
day="01",
volume="22",
number="3",
pages="177--210",
abstract="Real-world data is never perfect and can often suffer from corruptions (noise) that may impact interpretations of the data, models created from the data and decisions made based on the data. Noise can reduce system performance in terms of classification accuracy, time in building a classifier and the size of the classifier. Accordingly, most existing learning algorithms have integrated various approaches to enhance their learning abilities from noisy environments, but the existence of noise can still introduce serious negative impacts. A more reasonable solution might be to employ some preprocessing mechanisms to handle noisy instances before a learner is formed. Unfortunately, rare research has been conducted to systematically explore the impact of noise, especially from the noise handling point of view. This has made various noise processing techniques less significant, specifically when dealing with noise that is introduced in attributes. In this paper, we present a systematic evaluation on the effect of noise in machine learning. Instead of taking any unified theory of noise to evaluate the noise impacts, we differentiate noise into two categories: class noise and attribute noise, and analyze their impacts on the system performance separately. Because class noise has been widely addressed in existing research efforts, we concentrate on attribute noise. We investigate the relationship between attribute noise and classification accuracy, the impact of noise at different attributes, and possible solutions in handling attribute noise. Our conclusions can be used to guide interested readers to enhance data quality by designing various noise handling mechanisms."
}

@ARTICLE{Liu_2002, 
author={Xiaohui Liu and Gongxian Cheng and J. X. Wu}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Analyzing outliers cautiously}, 
year={2002}, 
volume={14}, 
number={2}, 
pages={432-437}, 
keywords={data mining;knowledge based systems;measurement errors;medical computing;self-organising feature maps;AI modeling;domain knowledge;glaucoma;knowledge-based system;measurement errors;noise modeling;outliers;self-organizing maps;visual impairments;Biomedical engineering;Finance;Measurement errors},
month={Mar}
}

@article{Lachenbruch_1966,
author = { Peter A. Lachenbruch },
title = {Discriminant Analysis When the Initial Samples Are Misclassified},
journal = {Technometrics},
volume = {8},
number = {4},
pages = {657-662},
year  = {1966},
publisher = {Taylor & Francis}
}

@article{Lachenbruch_1979,
author = { Peter A.  Lachenbruch },
title = {Note on Initial Misclassification Effects on the Quadratic Discriminant Function},
journal = {Technometrics},
volume = {21},
number = {1},
pages = {129-132},
year  = {1979},
publisher = {Taylor & Francis}
}

@article{Michalek_1980,

 abstract = {This article investigates the effect of misclassification and measurement error in the basic data on the asymptotic bias and efficiency of the logistic regression (LR) and normal discrimination (ND) classification procedures. The effect of misclassification in a single binary independent variable on the bias and efficiency of both procedures is also presented. Typically, asymptotic bias increases and efficiency decreases as misclassification and measurement error increase. The performance of LR relative to ND is shown to be better in the presence of error than without error.},
 author = {Joel E. Michalek and Ram C. Tripathi},
 journal = {Journal of the American Statistical Association},
 number = {371},
 pages = {713--721},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {The Effect of Errors in Diagnosis and Measurement on the Estimation of the Probability of an Event},
 volume = {75},
 year = {1980}
}

@article{Bi_2010,
title = "The efficiency of logistic regression compared to normal discriminant analysis under class-conditional classification noise",
journal = "Journal of Multivariate Analysis",
volume = "101",
number = "7",
pages = "1622 - 1637",
year = "2010",
author = "Yingtao Bi and Daniel R. Jeske",
keywords = "Class noise, Misclassification rate, Misspecified model, Asymptotic distribution"
}

@Inbook{Heskes2000,
author="Heskes, Tom",
title="The Use of Being Stubborn and Introspective",
bookTitle="Prerational Intelligence: Adaptive Behavior and Intelligent Systems Without Symbols and Logic, Volume 1, Volume 2 Prerational Intelligence: Interdisciplinary Perspectives on the Behavior of Natural and Artificial Systems, Volume 3",
year="2000",
publisher="Springer Netherlands",
address="Dordrecht",
pages="1184--1200",
abstract="Like humans, artificial neural networks can learn from examples. In supervised learning, an example consists of a ``question'' and its ``answer.'' The answer is provided by a ``teacher'' and used by the ``student'' to update her2 internal representation, i.e. the values of her neural weights. Using techniques from statistical mechanics, many properties arising from such a simple scheme have been investigated. The one-layered perceptron is a popular platform for these studies: it is complicated enough to arrive at interesting results, yet simple enough to be described analytically. Excellent reviews of the main achievements can be found in Seung et al. (1992), and Watkin et al. (1993)."
}

@article{Sanchez_1997,
title = "Prototype selection for the nearest neighbour rule through proximity graphs",
journal = "Pattern Recognition Letters",
volume = "18",
number = "6",
pages = "507 - 513",
year = "1997",
author = "J.S. Sánchez and F. Pla and F.J. Ferri",
keywords = "Nearest neighbour, Prototype selection, Gabriel graph, Relative neighbourhood graph"
}

@Article{Nettleton_2010,
author="Nettleton, David F.
and Orriols-Puig, Albert
and Fornells, Albert",
title="A study of the effect of different types of noise on the precision of supervised learning techniques",
journal="Artificial Intelligence Review",
year="2010",
month="Apr",
day="01",
volume="33",
number="4",
pages="275--306",
abstract="Machine learning techniques often have to deal with noisy data, which may affect the accuracy of the resulting data models. Therefore, effectively dealing with noise is a key aspect in supervised learning to obtain reliable models from data. Although several authors have studied the effect of noise for some particular learners, comparisons of its effect among different learners are lacking. In this paper, we address this issue by systematically comparing how different degrees of noise affect four supervised learners that belong to different paradigms. Specifically, we consider the Na{\"i}ve Bayes probabilistic classifier, the C4.5 decision tree, the IBk instance-based learner and the SMO support vector machine. We have selected four methods which enable us to contrast different learning paradigms, and which are considered to be four of the top ten algorithms in data mining (Yu et al. 2007). We test them on a collection of data sets that are perturbed with noise in the input attributes and noise in the output class. As an initial hypothesis, we assign the techniques to two groups, NB with C4.5 and IBk with SMO, based on their proposed sensitivity to noise, the first group being the least sensitive. The analysis enables us to extract key observations about the effect of different types and degrees of noise on these learning techniques. In general, we find that Na{\"i}ve Bayes appears as the most robust algorithm, and SMO the least, relative to the other two techniques. However, we find that the underlying empirical behavior of the techniques is more complex, and varies depending on the noise type and the specific data set being processed. In general, noise in the training data set is found to give the most difficulty to the learners."
}

@InProceedings{Prem_2004,
author="Melville, Prem
and Shah, Nishit
and Mihalkova, Lilyana
and Mooney, Raymond J.",
title="Experiments on Ensembles with Missing and Noisy Data",
booktitle="Multiple Classifier Systems",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="293--302",
abstract="One of the potential advantages of multiple classifier systems is an increased robustness to noise and other imperfections in data. Previous experiments on classification noise have shown that bagging is fairly robust but that boosting is quite sensitive. Decorate is a recently introduced ensemble method that constructs diverse committees using artificial data. It has been shown to generally outperform both boosting and bagging when training data is limited. This paper compares the sensitivity of bagging, boosting, and Decorate to three types of imperfect data: missing features, classification noise, and feature noise. For missing data, Decorate is the most robust. For classification noise, bagging and Decorate are both robust, with bagging being slightly better than Decorate, while boosting is quite sensitive. For feature noise, all of the ensemble methods increase the resilience of the base classifier."
}

@InProceedings{McDonald_2003,
author="McDonald, Ross A. and Hand, David J. and Eckley, Idris A.",
title="An Empirical Comparison of Three Boosting Algorithms on Real Data Sets with Artificial Class Noise",
booktitle="Multiple Classifier Systems",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="35--44",
abstract="Boosting algorithms are a means of building a strong ensemble classifier by aggregating a sequence of weak hypotheses. In this paper we consider three of the best-known boosting algorithms: Adaboost [9], Logitboost [11] and Brownboost [8]. These algorithms are adaptive, and work by maintaining a set of example and class weights which focus the attention of a base learner on the examples that are hardest to classify. We conduct an empirical study to compare the performance of these algorithms, measured in terms of overall test error rate, on five real data sets. The tests consist of a series of cross-validatory samples. At each validation, we set aside one third of the data chosen at random as a test set, and fit the boosting algorithm to the remaining two thirds, using binary stumps as a base learner. At each stage we record the final training and test error rates, and report the average errors within a 95{\%} confidence interval. We then add artificial class noise to our data sets by randomly reassigning 20{\%} of class labels, and repeat our experiment. We find that Brownboost and Logitboost prove less likely than Adaboost to overfit in this circumstance."
}

@Article{Quinlan1986,
author="Quinlan, J. R.",
title="Induction of decision trees",
journal="Machine Learning",
year="1986",
month="Mar",
day="01",
volume="1",
number="1",
pages="81--106",
abstract="The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions."
}

@article{Brodley_1999,
 author = {Brodley, Carla E. and Friedl, Mark A.},
 title = {Identifying Mislabeled Training Data},
 journal = {J. Artif. Int. Res.},
 issue_date = {July 1999},
 volume = {11},
 number = {1},
 month = jul,
 year = {1999},
 pages = {131--167},
 numpages = {37},
 acmid = {3013548},
 publisher = {AI Access Foundation},
 address = {USA}
} 

@Article{Libralon2009,
author="Libralon, Giampaolo Luiz
and de Carvalho, Andr{\'e} Carlos Ponce de Leon Ferreira
and Lorena, Ana Carolina",
title="Pre-processing for noise detection in gene expression classification data",
journal="Journal of the Brazilian Computer Society",
year="2009",
month="Mar",
day="01",
volume="15",
number="1",
pages="3--11",
abstract="Due to the imprecise nature of biological experiments, biological data is often characterized by the presence of redundant and noisy data. This may be due to errors that occurred during data collection, such as contaminations in laboratorial samples. It is the case of gene expression data, where the equipments and tools currently used frequently produce noisy biological data. Machine Learning algorithms have been successfully used in gene expression data analysis. Although many Machine Learning algorithms can deal with noise, detecting and removing noisy instances from the training data set can help the induction of the target hypothesis. This paper evaluates the use of distance-based pre-processing techniques for noise detection in gene expression data classification problems. This evaluation analyzes the effectiveness of the techniques investigated in removing noisy data, measured by the accuracy obtained by different Machine Learning classifiers over the pre-processed data."
}

@Article{Segata2010,
author="Segata, Nicola
and Blanzieri, Enrico
and Delany, Sarah Jane
and Cunningham, P{\'a}draig",
title="Noise reduction for instance-based learning with a local maximal margin approach",
journal="Journal of Intelligent Information Systems",
year="2010",
month="Oct",
day="01",
volume="35",
number="2",
pages="301--331",
abstract="To some extent the problem of noise reduction in machine learning has been finessed by the development of learning techniques that are noise-tolerant. However, it is difficult to make instance-based learning noise tolerant and noise reduction still plays an important role in k-nearest neighbour classification. There are also other motivations for noise reduction, for instance the elimination of noise may result in simpler models or data cleansing may be an end in itself. In this paper we present a novel approach to noise reduction based on local Support Vector Machines (LSVM) which brings the benefits of maximal margin classifiers to bear on noise reduction. This provides a more robust alternative to the majority rule on which almost all the existing noise reduction techniques are based. Roughly speaking, for each training example an SVM is trained on its neighbourhood and if the SVM classification for the central example disagrees with its actual class there is evidence in favour of removing it from the training set. We provide an empirical evaluation on 15 real datasets showing improved classification accuracy when using training data edited with our method as well as specific experiments regarding the spam filtering application domain. We present a further evaluation on two artificial datasets where we analyse two different types of noise (Gaussian feature noise and mislabelling noise) and the influence of different class densities. The conclusion is that LSVM noise reduction is significantly better than the other analysed algorithms for real datasets and for artificial datasets perturbed by Gaussian noise and in presence of uneven class densities."
}


@InProceedings{Segata_2009,
author="Segata, Nicola
and Blanzieri, Enrico
and Cunningham, P{\'a}draig",
title="A Scalable Noise Reduction Technique for Large Case-Based Systems",
booktitle="Case-Based Reasoning Research and Development",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="328--342",
abstract="Because case-based reasoning (CBR) is instance-based, it is vulnerable to noisy data. Other learning techniques such as support vector machines (SVMs) and decision trees have been developed to be noise-tolerant so a certain level of noise in the data can be condoned. By contrast, noisy data can have a big impact in CBR because inference is normally based on a small number of cases. So far, research on noise reduction has been based on a majority-rule strategy, cases that are out of line with their neighbors are removed. We depart from that strategy and use local SVMs to identify noisy cases. This is more powerful than a majority-rule strategy because it explicitly considers the decision boundary in the noise reduction process. In this paper we provide details on how such a local SVM strategy for noise reduction can be made scale to very large datasets (> 500,000 training samples). The technique is evaluated on nine very large datasets and shows excellent performance when compared with alternative techniques."
}

@Article{Zhang_2006,
author="W. Zhang and R. Rekaya and K. Bertrand",
title="A method for predicting disease subtypes in presence of misclassification among training samples using gene expression: Application to human breast cancer",
journal="Bioinformatics",
year="2006",
volume="22",
number="3",
pages="317–325",
}

@InProceedings{Cuendet_2008,
author="Cuendet, S{\'e}bastien
and Hakkani-T{\"u}r, Dilek
and Shriberg, Elizabeth",
title="Automatic Labeling Inconsistencies Detection and Correction for Sentence Unit Segmentation in Conversational Speech",
booktitle="Machine Learning for Multimodal Interaction",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="144--155",
}

@ARTICLE{Jaromczyk_1992, 
author={J. W. Jaromczyk and G. T. Toussaint}, 
journal={Proceedings of the IEEE}, 
title={Relative neighborhood graphs and their relatives}, 
year={1992}, 
volume={80}, 
number={9}, 
pages={1502-1517}, 
keywords={computational geometry;computer vision;pattern recognition;spatial data structures;visual databases;computational morphology;computer vision;databases;neighborhood graphs;pattern classification;spatial analysis;Application software;Bibliographies;Biology computing;Computational geometry;Computer applications;Computer science;Computer vision;Morphology;Pattern analysis;Shape}, 
month={Sep}
}

@INPROCEEDINGS{Du_2010, 
author={W. Du and K. Urahama}, 
booktitle={2010 2nd International Symposium on Aware Computing}, 
title={Error-correcting semi-supervised pattern recognition with mode filter on graphs}, 
year={2010}, 
volume={}, 
number={}, 
pages={6-11}, 
keywords={filtering theory;graph theory;image segmentation;iterative methods;learning (artificial intelligence);optimisation;pattern recognition;computational speeds;direct solution method;error correcting semi supervised pattern recognition;graphs filters;impulsive noises;iterative methods;nonlinear optimization;smoothing images;Iris},
month={Nov}
}

@InProceedings{Lallich_2002,
author="Lallich, St{\'e}phane
and Muhlenbach, Fabrice
and Zighed, Djamel A.",
title="Improving Classification by Removing or Relabeling Mislabeled Instances",
booktitle="Foundations of Intelligent Systems",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="5--15",
abstract="It is common that a database contains noisy data. An important source of noise consists in mislabeled training instances. We present a new approach that deals with improving classification accuracies in such a case by using a preliminary filtering procedure. An example is suspect when in its neighborhood defined by a geometrical graph the proportion of examples of the same class is not significantly greater than in the whole database. Such suspect examples in the training data can be removed or relabeled. The filtered training set is then provided as input to learning algorithm. Our experiments on ten benchmarks of UCI Machine Learning Repository using 1-NN as the final algorithm show that removing give better results than relabeling. Removing allows maintaining the generalization error rate when we introduce from 0 to 20{\%} of noise on the class, especially when classes are well separable."
}

@INPROCEEDINGS{Breve_2010, 
author={F. A. Breve and L. Zhao and M. G. Quiles}, 
booktitle={The 2010 International Joint Conference on Neural Networks (IJCNN)}, 
title={Semi-supervised learning from imperfect data through particle cooperation and competition}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-8}, 
keywords={cooperative systems;data mining;learning (artificial intelligence);numerical analysis;pattern classification;reliability;competitive mechanisms;cooperative mechanisms;labels reliability;major classification error;numerical analysis;particle cooperation;particle walk semisupervised learning method;robust machine learning technique;wrong label contamination;Robustness}, 
month={July}
}

@Article{Guan2011,
author="Guan, Donghai
and Yuan, Weiwei
and Lee, Young-Koo
and Lee, Sungyoung",
title="Identifying mislabeled training data with the aid of unlabeled data",
journal="Applied Intelligence",
year="2011",
month="Dec",
day="01",
volume="35",
number="3",
pages="345--358",
abstract="This paper presents a new approach for identifying and eliminating mislabeled training instances for supervised learning algorithms. The novelty of this approach lies in the using of unlabeled instances to aid the detection of mislabeled training instances. This is in contrast with existing methods which rely upon only the labeled training instances. Our approach is straightforward and can be applied to many existing noise detection methods with only marginal modifications on them as required. To assess the benefit of our approach, we choose two popular noise detection methods: majority filtering (MF) and consensus filtering (CF). MFAUD/CFAUD is the new proposed variant of MF/CF which relies on our approach and denotes majority/consensus filtering with the aid of unlabeled data. Empirical study validates the superiority of our approach and shows that MFAUD and CFAUD can significantly improve the performances of MF and CF under different noise ratios and labeled ratios. In addition, the improvement is more remarkable when the noise ratio is greater."
}

@ARTICLE{Li_2012, 
author={C. H. Li and B. C. Kuo and C. T. Lin and C. S. Huang}, 
journal={IEEE Transactions on Geoscience and Remote Sensing}, 
title={A Spatial Contextual Support Vector Machine for Remotely Sensed Image Classification}, 
year={2012}, 
volume={50}, 
number={3}, 
pages={784-799}, 
keywords={Markov processes;image classification;remote sensing;Bayesian contextual classifier;DC mall data;Indian Pine site;Markov random fields;Washington;context-sensitive semisupervised SVM;hyperspectral image classification techniques;hyperspectral images;k nearest neighbor classifier;maximum likelihood classifier;remotely sensed image classification;spatial-contextual support vector machine;Accuracy;Hyperspectral imaging;Kernel;Markov processes;Support vector machines;Training;Classification;Markov random fields (MRFs);spatial–contextual information;support vector machines (SVMs)}, 
month={March}
}

@INPROCEEDINGS{Duan_2009, 
author={Y. Duan and Y. Gao and X. Ren and H. Che and K. Yang}, 
booktitle={2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery}, 
title={Semi-supervised Classification and Noise Detection}, 
year={2009}, 
volume={1}, 
number={}, 
pages={277-280}, 
keywords={learning (artificial intelligence);data classification;label propagation algorithm;noise detection;semisupervised classification;Clustering algorithms;Computer science;Data mining;Distributed computing;Educational institutions;Fuzzy systems;Probability distribution;Semisupervised learning;Support vector machine classification;Support vector machines;label propagation;noise detection;semi-supervised classification}, 
month={Aug}
}

@InProceedings{Anastasia_2008,
author="Krithara, Anastasia
and Amini, Massih R.
and Renders, Jean-Michel
and Goutte, Cyril",

title="Semi-supervised Document Classification with a Mislabeling Error Model",
booktitle="Advances in Information Retrieval",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="370--381",
abstract="This paper investigates a new extension of the Probabilistic Latent Semantic Analysis (PLSA) model [6] for text classification where the training set is partially labeled. The proposed approach iteratively labels the unlabeled documents and estimates the probabilities of its labeling errors. These probabilities are then taken into account in the estimation of the new model parameters before the next round. Our approach outperforms an earlier semi-supervised extension of PLSA introduced by [9] which is based on the use of fake labels. However, it maintains its simplicity and ability to solve multiclass problems. In addition, it gives valuable information about the most uncertain and difficult classes to label. We perform experiments over the 20Newsgroups, WebKB and Reuters document collections and show the effectiveness of our approach over two other semi-supervised algorithms applied to these text classification problems."
}

@article{watts98,
author = {D.J. Watts and S. Strogatz},
journal = {Nature},
keywords = {networks},
mendeley-groups = {Social\_Dynamics,ComplexNetwork},
number = {6684},
pages = {440--442},
title = {Collective dynamics of 'small-world' networks},
volume = {393},
year = {1998}
}

@ARTICLE{brinEpage1998,
  author = {Brin, S. and Page, L.},
  title = {The Anatomy of a Large-Scale Hypertextual Web Search Engine},
  journal = {Computer Networks and ISDN Systems},
  year = {1998},
  volume = {V},
  pages = {107--117},
  eid = {Elsevier Science Publishers},
}


